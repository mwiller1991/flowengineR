---
title: "In Detail: Evaluation Engines"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{In Detail: Evaluation Engines}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Evaluation Engines in `flowengineR` are used to assess model output. They compute and return evaluation results for different purposes, such as:

- **General summary statistics** (e.g., mean, standard deviation)
- **Prediction accuracy** (e.g., mean squared error)
- **Fairness metrics** (e.g., statistical parity)

These engines operate on raw or adjusted predictions and return standardized result formats that integrate into reporting and downstream aggregation.

---

## üì• Standardized Inputs (via Wrapper)
These inputs are passed to the engine wrapper and are provided directly by the workflow:

| Input              | Type             | Description                                                       |
|--------------------|------------------|-------------------------------------------------------------------|
| `eval_data`        | data.frame       | Data containing predictions and reference values (‚ö†Ô∏è provided by the framework) |
| `params`           | list             | Additional parameters for the evaluation                          |
| `protected_name`   | character        | Name(s) of protected attributes (‚ö†Ô∏è provided by the framework)  |

Framework resolution:
- `eval_data`: provided by the framework 
- `params`: from `control$params$evaluation$params`
- `protected_name`: ‚ö†resolved via `control$data$vars$protected`

---

## üì§ Standardized Output (via Wrapper)
Returned via `initialize_output_eval()`:

| Output              | Type         | Description                                                       |
|---------------------|--------------|-------------------------------------------------------------------|
| `metrics`           | list         | Named list of metric values                                       |
| `eval_type`         | character    | Type of evaluation (e.g., "performance", "fairness")              |
| `input_data`        | data.frame   | Input data used for evaluation                                    |
| `protected_attributes` | data.frame | Protected variables, if applicable                                |
| `params`            | list         | Parameters used in the evaluation                                 |
| `specific_output`   | list / NULL  | Optional additional outputs specific to the engine                |

---

## ‚öôÔ∏è Available Evaluation Engines

| Engine Name              | Function Name                           | Description                                    | Template Path                                                                  | Detail Page |
|--------------------------|------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------|--------------|
| `evaluate_mse`           | `engine_evaluation_mse()`               | Mean squared error evaluation                  | `inst/templates_control/7_2_a_template_control_evaluation_mse.R`                 | [üìÑ](detail_engine_evaluation_mse.html) |
| `evaluation_statisticalparity`       | `engine_evaluation_statisticalparity()` | Statistical parity difference (SPD)            | `inst/templates_control/7_3_a_template_control_evaluation_statisticalparity.R`   | [üìÑ](detail_engine_evaluation_statisticalparity.html) |
| `evaluate_summarystats`  | `engine_evaluation_summarystats()`      | Summary statistics (mean, sd, etc.)            | `inst/templates_control/7_1_a_template_control_evaluation_summarystats.R`        | [üìÑ](detail_engine_evaluation_summarystats.html) |

Use `list_registered_engines("evaluation")` to view all currently loaded evaluation engines.

---

## üß© Engine Usage in Workflow

```r
control$engine_select$evaluation <- "evaluate_mse"
control$params$evaluation <- controller_evaluation(
  params = list()
)
```

Evaluation engines are called after predictions are finalized. They provide essential performance and fairness diagnostics used for result interpretation and reporting.

---

## üõ†Ô∏è Writing Your Own Evaluation Engine

To create a custom evaluation engine:

1. Implement `engine_*()` to compute and return metrics.
2. Create a `wrapper_*()` to manage parameters and structure.
3. Return standardized results using `initialize_output_eval()`.

---

## üéØ Possible Use-Case: Combined Metric Tracking

You can include multiple evaluation engines in your control setup to track both performance and fairness indicators. Each engine receives its own parameter list within the evaluation controller.

Example:

```r
control$engine_select$evaluation <- c("evaluate_mse", "evaluate_statpar")
control$params$evaluation <- controller_evaluation(
  params = list(
    eval_mse = list(weighting_factor = 0.5),          # Example parameter for MSE
    eval_statisticalparity = list(threshold = 0.1)    # Example parameter for SPD
  )
)
```

This modular structure allows comparisons across metrics and supports flexible reporting.
